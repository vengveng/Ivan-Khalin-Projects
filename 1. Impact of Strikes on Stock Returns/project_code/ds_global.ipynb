{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from linearmodels.panel import PanelOLS\n",
    "from pandas.tseries.offsets import BDay\n",
    "from rapidfuzz import process\n",
    "\n",
    "\n",
    "def get_file_path(relative_path):\n",
    "    parent_dir = os.getcwd()\n",
    "    return os.path.join(parent_dir, relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(date_str):\n",
    "\n",
    "    for fmt in ('%d.%m.%Y', '%d/%m/%Y', '%Y-%m-%d'):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        \n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return pd.NaT\n",
    "\n",
    "actual_index = 'Company Name'\n",
    "approximate_index = 'Employer'\n",
    "start_date_index = 'Start Date'\n",
    "isin_index = 'ISIN Code'\n",
    "ipo_date_index = 'IPO Date'\n",
    "\n",
    "original_path = get_file_path('data/all_us.csv')\n",
    "test_path = get_file_path('GLOBAL_TEST.csv')\n",
    "dirty_data = get_file_path('work_stoppages.csv')\n",
    "\n",
    "actual = pd.read_csv(original_path, dtype=str)\n",
    "actual_list = actual[actual_index].tolist()\n",
    "\n",
    "# Dirty dataset\n",
    "approximate = pd.read_csv(dirty_data, dtype=str)\n",
    "approximate['Start Date'] = pd.to_datetime(approximate['Start Date'], format='%Y-%m-%d')\n",
    "approximate['End Date'] = pd.to_datetime(approximate['End Date'], format='%Y-%m-%d')\n",
    "approximate['Request Start Date'] = approximate['Start Date'] - pd.DateOffset(years=1)\n",
    "approximate['Request End Date'] = approximate['End Date'] + pd.DateOffset(years=1)\n",
    "approximate['Duration'] = approximate['End Date'] - approximate['Start Date']\n",
    "\n",
    "# Attempt to convert date strings to datetime \n",
    "actual['IPO Date'] = actual['IPO Date'].apply(parse_dates)\n",
    "\n",
    "# Handle exceptions, use incorporation dates as proxy for ipo dates\n",
    "nat_mask = actual['IPO Date'].isna()\n",
    "nat_rows = actual[nat_mask].copy()\n",
    "nat_rows['IPO Date'] = nat_rows['Date of Incorporation'].apply(parse_dates)\n",
    "actual.loc[nat_mask, 'IPO Date'] = nat_rows['IPO Date']\n",
    "\n",
    "approximate['isin'] = np.nan\n",
    "\n",
    "rows_to_drop = []\n",
    "new_rows = []\n",
    "\n",
    "# Matching company names to ISIN codes with fuzzywuzzy\n",
    "for index, row in approximate.iterrows():\n",
    "\n",
    "    company = row[approximate_index]\n",
    "    match = None\n",
    "    \n",
    "    try:\n",
    "        match = process.extractOne(company, actual_list, score_cutoff=90)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if match:\n",
    "\n",
    "        company_name = match[0]\n",
    "        extraction = actual.loc[actual[actual_index] == company_name, (isin_index, ipo_date_index)].iloc[0]\n",
    "    \n",
    "        ipo_date = extraction.iloc[1]\n",
    "        strike_date = row[start_date_index]\n",
    "        \n",
    "        if strike_date > ipo_date:\n",
    "\n",
    "            isin_number = extraction.iloc[0]\n",
    "            rsd = row['Request Start Date']\n",
    "            red = row['Request End Date']\n",
    "            ni = row['# Idled']\n",
    "            dur = row['Duration'].days\n",
    "            ind = row['Industry']\n",
    "\n",
    "            actual.loc[index, approximate_index] = company_name\n",
    "            actual.loc[index, 'isin'] = isin_number\n",
    "\n",
    "            \n",
    "            # print(company_name, isin_number, strike_date.strftime('%m/%d/%Y'), index+2, match[1])\n",
    "            new_row = {'Company Name': company_name, 'ISIN': isin_number, 'Industry': ind, '# Idled': ni, 'Start Date': strike_date.strftime('%m/%d/%Y'), 'Duration': dur, 'Request Start Date': rsd, 'Request End Date': red, 'Original Index': index+2}\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "\n",
    "        else:\n",
    "            rows_to_drop.append(index)\n",
    "\n",
    "    else:\n",
    "        rows_to_drop.append(index)\n",
    "\n",
    "# print(rows_to_drop)\n",
    "approximate.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "new_data = pd.DataFrame(new_rows)\n",
    "\n",
    "new_data.to_csv('data/work_stoppages_showcase.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was then used to pas refinitif eikon request in CEDIF. A column specifying the request reception date was added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various data were obtained and placed into .csv files to be used in the section that follows. Here we combine and process the data such that it can be turned into a panel dataset. We opt to save the intermittent results to .csv files for easy troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/y4zzq7f116jdmpzf6b0_rqn00000gn/T/ipykernel_28013/1462044621.py:113: UserWarning: Parsing dates in DD/MM/YYYY format when dayfirst=False (the default) was specified. This may lead to inconsistently parsed dates! Specify a format to ensure consistent parsing.\n",
      "  data['Request Start Date'] = pd.to_datetime(data['Request Start Date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.13%  "
     ]
    }
   ],
   "source": [
    "# Tool functions\n",
    "def adjust_to_business_day(date, data_index):\n",
    "\n",
    "    original_date = date \n",
    "    max_attempts = 10 \n",
    "    attempts = 0\n",
    "\n",
    "    while date not in data_index and attempts < max_attempts:\n",
    "        date += BDay()\n",
    "        attempts += 1\n",
    "\n",
    "    if attempts == max_attempts:\n",
    "\n",
    "        print(f\"Couldn't find a suitable business day for original date: {original_date}\")\n",
    "        raise ValueError(\"Couldn't find a suitable business day in the index after 10 attempts.\")\n",
    "    \n",
    "    return date\n",
    "\n",
    "def open_csv(filepath):\n",
    "    try:\n",
    "        return pd.read_csv(filepath)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "def load_and_prepare_data(filepath, skiprows=1, date_col='Date', date_format='%Y', sort_index=True):\n",
    "    df = pd.read_csv(filepath, skiprows=skiprows)\n",
    "    df.rename(columns={df.columns[0]: date_col}, inplace=True)\n",
    "    df[date_col] = pd.to_datetime(df[date_col], format=date_format)\n",
    "    df.set_index(date_col, inplace=True)\n",
    "    # if sort_index:\n",
    "    df.sort_index(inplace=True)\n",
    "    return df[~df.index.isna()]\n",
    "\n",
    "def get_data_for_isin_and_date(df, isin, date):\n",
    "    data = None\n",
    "    closest_year = df.index.asof(date)\n",
    "    try:\n",
    "        data = df.loc[closest_year, isin]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "def prepare_data(df):\n",
    "    df = df.drop(df.columns[:2], axis=1).T.reset_index()\n",
    "    df.columns = ['Date', 'Price']\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    return df.dropna(subset=['Date']).set_index('Date')\n",
    "\n",
    "def process_and_save_df(df, file_path, mask_all_nan):\n",
    "    df_processed = df[~mask_all_nan].fillna(0).reset_index()\n",
    "    df_processed = df_processed.rename(columns={'index': 'Date'})\n",
    "    file_path = get_file_path(file_path)\n",
    "    df_processed.to_csv(file_path, index=False)\n",
    "\n",
    "# Logic functions\n",
    "def changes(df, start_date, end_date, isin) -> dict:\n",
    "\n",
    "    df_index = df.index\n",
    "\n",
    "    # In this section more factors can be added\n",
    "    workers = get_data_for_isin_and_date(emp, isin, start_date)\n",
    "    # print(workers)\n",
    "    if workers is None or pd.isna(workers):\n",
    "        workers = 0\n",
    "\n",
    "    capital = get_data_for_isin_and_date(cap, isin, start_date)\n",
    "    if capital is None:\n",
    "        capital = 0\n",
    "\n",
    "    # Start/End dates with a month offset\n",
    "    start_date = adjust_to_business_day(start_date - pd.DateOffset(months=1), df_index)\n",
    "    end_date = adjust_to_business_day(end_date + pd.DateOffset(months=1), df_index)\n",
    "\n",
    "    # Fetch the stock prices for the period, calulate returns\n",
    "    price_slice = df.loc[start_date:end_date, 'Price'].pct_change()\n",
    "    price_slice.replace(0, np.nan, inplace=True)\n",
    "    price_slice.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Create a series as long as the price_slice period\n",
    "    workers_slice = pd.Series(workers, index=price_slice.index)\n",
    "    capital_slice = pd.Series(capital, index=price_slice.index)\n",
    "    # print(workers_slice)\n",
    "\n",
    "    isin_str = str(isin)\n",
    "\n",
    "    # The output dictionary, add factors here with key-value pairs, keys are not important\n",
    "    # Adjust the for loop to handle additional data right after the get_y() function is invoced\n",
    "    return {isin_str: price_slice, 'workers': workers_slice, 'capital': capital_slice}\n",
    "\n",
    "def get_y(isin, index, start_date, end_date):\n",
    "    filepath = f'all_csv/{isin.upper()}_{index}.csv'\n",
    "    df = open_csv(get_file_path(filepath))\n",
    "    if df is None:\n",
    "        return None\n",
    "    df_prices = prepare_data(df)\n",
    "    return changes(df_prices, start_date, end_date, isin)\n",
    "\n",
    "def get_x(idle, duration, industry):\n",
    "    # DEPRECATED\n",
    "    return [idle, duration, industry]\n",
    "\n",
    "# Main sequence\n",
    "# Loading data\n",
    "emp_path = 'data/ds_employees.csv'\n",
    "cap_path = 'data/ds_cintensity.csv'\n",
    "filepath = 'data/work_stoppages_extracted.csv'\n",
    "sp_path = 'data/S&P_ds.csv'\n",
    "\n",
    "emp = load_and_prepare_data(get_file_path(emp_path))\n",
    "cap = load_and_prepare_data(get_file_path(cap_path))\n",
    "\n",
    "data = pd.read_csv(get_file_path(filepath))\n",
    "data['Request Start Date'] = pd.to_datetime(data['Request Start Date'], errors='coerce')\n",
    "data['Request Recieved Date'] = pd.to_datetime(data['Request Start Date'], errors='coerce')\n",
    "data['Start Date'] = data['Request Start Date'] + pd.DateOffset(years=1)\n",
    "\n",
    "# Master date range index to allign the unbalanced pooled dataset\n",
    "master_date_range = pd.date_range(start='1982-1-1', end='2021-12-31', freq='D')\n",
    "all_slices_combined = pd.DataFrame(index=master_date_range)\n",
    "strike_dummy_slices = pd.DataFrame(index=master_date_range)\n",
    "relative_idle_slices = pd.DataFrame(index=master_date_range)\n",
    "c_intesity_slices = pd.DataFrame(index=master_date_range)\n",
    "\n",
    "leadup_slices = pd.DataFrame(index=master_date_range)\n",
    "followup_slices = pd.DataFrame(index=master_date_range)\n",
    "\n",
    "# S&P returns, special treatment round 1\n",
    "sp = pd.read_csv(get_file_path(sp_path), usecols=[0, 1], parse_dates=[0], dayfirst=True)\n",
    "sp.set_index('Date', inplace=True)\n",
    "sp = sp.pct_change()\n",
    "sp = sp.reindex(master_date_range)\n",
    "sp.replace(0, np.nan, inplace=True)\n",
    "sp.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Result list\n",
    "time_slices_info = []\n",
    "\n",
    "n = 0\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # For limitng recursion depth\n",
    "    # if n == 30:\n",
    "    #         break\n",
    "    # n += 1\n",
    "\n",
    "    # Can not continue if duration value is missing\n",
    "    try:\n",
    "        duration = int(row['Duration'])\n",
    "    except:\n",
    "        duration = None\n",
    "        continue\n",
    "    \n",
    "    # Fetching data for each entry\n",
    "    isin = row['ISIN']\n",
    "    idle = row['# Idled']\n",
    "    industry = row['Industry']\n",
    "    start_date = row['Start Date']\n",
    "    end_date = start_date + pd.DateOffset(days=duration)\n",
    "    request_date = row['Request Recieved Date']\n",
    "\n",
    "    # Each tuple in this list has all the data needed to call the get_y() function\n",
    "    # The list contains the tuples corresponding to each entry\n",
    "    time_slices_info.append((start_date, end_date, isin, index+1, int(idle)))\n",
    "\n",
    "\n",
    "for i in time_slices_info:\n",
    "    start_date, end_date, isin, index, idle = i\n",
    "    # Load the data for the time slice\n",
    "    slice_dict = get_y(isin, index, start_date, end_date)\n",
    "\n",
    "    # Dependent variable section\n",
    "    if slice_dict is None:\n",
    "            continue\n",
    "    (isin, slice_data), (z, total_employees), (zz, c_intesity)  = slice_dict.items()\n",
    "    relative_idle = idle / total_employees\n",
    "    relative_idle[np.isinf(relative_idle)] = 0\n",
    "    # print(relative_idle)\n",
    "\n",
    "    # Reindex the slice_data to the master date range\n",
    "    slice_reindexed = slice_data.reindex(master_date_range)\n",
    "    relative_idle = relative_idle.reindex(master_date_range)\n",
    "    c_intesity = c_intesity.reindex(master_date_range)\n",
    "    # print(relative_idle.isna().all())\n",
    "    \n",
    "    if isin in all_slices_combined.columns:\n",
    "        all_slices_combined[isin].update(slice_reindexed)\n",
    "    else:\n",
    "        all_slices_combined[isin] = slice_reindexed\n",
    "\n",
    "    # Dummy variable section, take in strike start-end dates\n",
    "    current_slice_range = pd.date_range(start=start_date, end=end_date)\n",
    "    # Set all values in that range to 1\n",
    "    dummy_series = pd.Series(1, index=current_slice_range)\n",
    "    # Align with master date index\n",
    "    dummy_series = dummy_series.reindex(master_date_range)\n",
    "\n",
    "    # Leadup dummy\n",
    "    leadup = start_date - pd.DateOffset(months=1, days=1)\n",
    "  \n",
    "    current_slice_range = pd.date_range(start=leadup, end=start_date)\n",
    "    leadup_dummy_series = pd.Series(1, index=current_slice_range)\n",
    "    leadup_dummy_series = leadup_dummy_series.reindex(master_date_range)\n",
    "    # leadup_dummy_series = leadup_dummy_series - dummy_series\n",
    "\n",
    "    # Followup dummy\n",
    "    followup_date = end_date + pd.DateOffset(months=1, days=1)\n",
    "    current_slice_range = pd.date_range(start=end_date, end=followup_date)\n",
    "    followup_dummy_series = pd.Series(1, index=current_slice_range)\n",
    "    followup_dummy_series = followup_dummy_series.reindex(master_date_range)\n",
    "\n",
    "    overlap_mask = (dummy_series == 1) & (leadup_dummy_series == 1)\n",
    "    leadup_dummy_series[overlap_mask] = 0\n",
    "\n",
    "    overlap_mask = (dummy_series == 1) & (followup_dummy_series == 1)\n",
    "    followup_dummy_series[overlap_mask] = 0\n",
    "\n",
    "    # If ISIN is not yet in the output dataframe, add ISIN and add the data\n",
    "    # If ISIN is already in the output dataframe, update the (missing) values\n",
    "    if isin in strike_dummy_slices.columns:\n",
    "        strike_dummy_slices[isin].update(dummy_series)\n",
    "    else:\n",
    "        strike_dummy_slices[isin] = dummy_series\n",
    "\n",
    "    if isin in relative_idle_slices.columns:\n",
    "        relative_idle_slices[isin].update(relative_idle)\n",
    "    else:\n",
    "        relative_idle_slices[isin] = relative_idle\n",
    "\n",
    "    if isin in c_intesity_slices.columns:\n",
    "        c_intesity_slices[isin].update(c_intesity)\n",
    "    else:\n",
    "        c_intesity_slices[isin] = c_intesity\n",
    "\n",
    "    if isin in leadup_slices.columns:\n",
    "        leadup_slices[isin].update(leadup_dummy_series)\n",
    "    else:\n",
    "        leadup_slices[isin] = leadup_dummy_series\n",
    "\n",
    "    if isin in followup_slices.columns:\n",
    "        followup_slices[isin].update(followup_dummy_series)\n",
    "    else:\n",
    "        followup_slices[isin] = followup_dummy_series\n",
    "    # print(relative_idle_slices.isna().all())\n",
    "\n",
    "    # Sections can be easily replicated\n",
    "    progress_value = index / len(data) * 100\n",
    "    print(f'\\rProgress: {progress_value:.2f}%  ', end='')\n",
    "\n",
    "# Create a filter mask that removes any entries with missing returns data\n",
    "mask_all_nan = all_slices_combined.isna().all(axis=1)\n",
    "\n",
    "# Save the data into separate files, apply the filter mask\n",
    "# process_and_save_df(all_slices_combined, 'dsf/data_for_analysis_panel.csv', mask_all_nan)\n",
    "process_and_save_df(strike_dummy_slices, 'data/data_for_analysis_panel_dummy.csv', mask_all_nan)\n",
    "process_and_save_df(relative_idle_slices, 'data/data_for_analysis_panel_relative_idle.csv', mask_all_nan)\n",
    "process_and_save_df(c_intesity_slices, 'data/data_for_analysis_panel_cintensity.csv', mask_all_nan)\n",
    "\n",
    "process_and_save_df(leadup_slices, 'data/data_for_analysis_panel_leadup_dummy.csv', mask_all_nan)\n",
    "process_and_save_df(followup_slices, 'data/data_for_analysis_panel_followup_dummy.csv', mask_all_nan)\n",
    "\n",
    "# S&P special treatment round 2, was not cooperating :/\n",
    "sp = sp[~mask_all_nan]\n",
    "# ATTENTION! Possibly wrongful forward filling of S&P returns #\n",
    "sp.fillna(method='ffill', inplace=True)\n",
    "###############################################################\n",
    "sp = sp.fillna(0)\n",
    "sp = sp.reset_index()\n",
    "sp.rename(columns={'index': 'Date'}, inplace=True)\n",
    "sp.rename(columns={'S&P500_CPI': 'spr'}, inplace=True)\n",
    "sp.to_csv(get_file_path('data/data_for_analysis_panel_sp.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding control variables and generating miscellaneous variables for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivankhalin/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: RuntimeWarning: invalid value encountered in accumulate\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "def process_data(read_path, save_path, calculation_func):\n",
    "    df = pd.read_csv(read_path, skiprows=1)\n",
    "    df = df.rename(columns={'ISIN codes':'Date'})\n",
    "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = calculation_func(df)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df[~df.index.duplicated()]\n",
    "    df = df[df.index.isin(master_date_range)]\n",
    "    df.to_csv(save_path)\n",
    "\n",
    "master_date_range = pd.date_range(start='1982-1-1', end='2021-12-31', freq='D')\n",
    "\n",
    "# Calulation functions\n",
    "def calculate_stock_returns(df):\n",
    "    return df.pct_change(fill_method='pad')\n",
    "\n",
    "def calculate_volatility(df):\n",
    "    return df.rolling(window=5).std()\n",
    "\n",
    "def calculate_volatility_of_volatility(df):\n",
    "    vol = df.rolling(window=5).std()\n",
    "    return vol.rolling(window=5).std()\n",
    "\n",
    "def calculate_cumulative_returns(df):\n",
    "    return (1 + df.pct_change(fill_method='pad')).cumprod() - 1\n",
    "\n",
    "def calculate_rolling_average(df):\n",
    "    return df.rolling(window=5).mean()\n",
    "\n",
    "def plain_get(df):\n",
    "    return df\n",
    "\n",
    "\n",
    "# File paths, respective calls\n",
    "file_paths = {\n",
    "    'stock_returns': {\n",
    "        'read_path': 'all_csv/stock_prices.csv', \n",
    "        'save_path': 'data/data_for_analysis_panel.csv', \n",
    "        'calculation': calculate_stock_returns\n",
    "    },\n",
    "    'volatility': {\n",
    "        'read_path': 'all_csv/stock_prices.csv', \n",
    "        'save_path': 'data/data_for_analysis_panel_5r_volatility.csv', \n",
    "        'calculation': calculate_volatility\n",
    "    },\n",
    "    'volatility_of_volatility': {\n",
    "        'read_path': 'all_csv/stock_prices.csv', \n",
    "        'save_path': 'data/data_for_analysis_panel_volatility_of_5r_volatility.csv', \n",
    "        'calculation': calculate_volatility_of_volatility\n",
    "    },\n",
    "    'cumulative_returns': {\n",
    "        'read_path': 'all_csv/stock_prices.csv', \n",
    "        'save_path': 'data/data_for_analysis_panel_cumulative_returns.csv', \n",
    "        'calculation': calculate_cumulative_returns\n",
    "    },\n",
    "    'rolling_average': {\n",
    "        'read_path': 'all_csv/stock_prices.csv', \n",
    "        'save_path': 'data/data_for_analysis_panel_5day_average.csv', \n",
    "        'calculation': calculate_rolling_average\n",
    "    },\n",
    "    'current_ratio': {\n",
    "        'read_path': 'all_csv/currentratio.csv', \n",
    "        'save_path': 'data/data_for_analysis_panel_current.csv', \n",
    "        'calculation': plain_get\n",
    "    },\n",
    "    'debt_to_equity': {\n",
    "        'read_path': 'all_csv/currentratio.csv',\n",
    "        'save_path': 'data/data_for_analysis_panel_d2e.csv', \n",
    "        'calculation': plain_get\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in file_paths.items():\n",
    "    target, parameters = i\n",
    "    read_path = get_file_path(parameters['read_path'])\n",
    "    save_path = get_file_path(parameters['save_path'])\n",
    "    calculation = parameters['calculation']\n",
    "    process_data(read_path, save_path, calculation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to combining our various .csv files into the long format panel dataset. Interestingly, we have found an easier way to retrieve stock prices for individual companies and filter them for the event windows which enabled us to circumvent the need to retrieve individual serial slices for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_path = 'data/data_for_analysis_panel.csv'\n",
    "stock_data = pd.read_csv(get_file_path(stock_path))\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "stock_data.set_index('Date', inplace=True)\n",
    "stock_data_transposed = stock_data.T\n",
    "stock_melted = stock_data_transposed.reset_index().melt(id_vars=['index'], var_name='Date', value_name='return')\n",
    "stock_melted.rename(columns={'index': 'ISIN'}, inplace=True)\n",
    "stock_melted.set_index(['ISIN', 'Date'], inplace=True)\n",
    "\n",
    "def process_and_merge_data(file_path, value_name, main_df):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data_transposed = data.T\n",
    "    data_melted = data_transposed.reset_index().melt(id_vars=['index'], var_name='Date', value_name=value_name)\n",
    "    data_melted.rename(columns={'index': 'ISIN'}, inplace=True)\n",
    "    \n",
    "    return main_df.merge(data_melted.set_index(['ISIN', 'Date']), left_index=True, right_index=True, how='left')\n",
    "\n",
    "def add_time_varying_factor(factor_file_path, factor_name, panel_data, date_col='Date'):\n",
    "   \n",
    "    factor_data = pd.read_csv(factor_file_path)\n",
    "    factor_data[date_col] = pd.to_datetime(factor_data[date_col])\n",
    "    factor_data.set_index(date_col, inplace=True)\n",
    "    panel_data_reset = panel_data.reset_index()\n",
    "\n",
    "    panel_data_with_factor = pd.merge(panel_data_reset, factor_data[[factor_name]], left_on=date_col, right_index=True, how='left')\n",
    "    panel_data_with_factor.set_index(['ISIN', date_col], inplace=True)\n",
    "\n",
    "    return panel_data_with_factor\n",
    "\n",
    "# Merging additional datasets with the main DataFrame\n",
    "dummy_path = get_file_path('data/data_for_analysis_panel_dummy.csv')\n",
    "ridle_path = get_file_path('data/data_for_analysis_panel_relative_idle.csv')\n",
    "cap_path = get_file_path('data/data_for_analysis_panel_cintensity.csv')\n",
    "sp_path = get_file_path('data/data_for_analysis_panel_sp.csv')\n",
    "currentsave = get_file_path('data/data_for_analysis_panel_current.csv')\n",
    "d2esave = get_file_path('data/data_for_analysis_panel_d2e.csv')\n",
    "leadup_dummy = get_file_path('data/data_for_analysis_panel_leadup_dummy.csv')\n",
    "followup_dummy = get_file_path('data/data_for_analysis_panel_followup_dummy.csv')\n",
    "rv = get_file_path('data/data_for_analysis_panel_5r_volatility.csv')\n",
    "vrv = get_file_path('data/data_for_analysis_panel_volatility_of_5r_volatility.csv')\n",
    "cr = get_file_path('data/data_for_analysis_panel_cumulative_returns.csv')\n",
    "ra = get_file_path('data/data_for_analysis_panel_5day_average.csv')\n",
    "\n",
    "panel_data = process_and_merge_data(dummy_path, 'strike', stock_melted)\n",
    "panel_data = process_and_merge_data(ridle_path, 'ridle', panel_data)\n",
    "panel_data = process_and_merge_data(cap_path, 'cintensity', panel_data)\n",
    "panel_data = process_and_merge_data(currentsave, 'cratio', panel_data)\n",
    "panel_data = process_and_merge_data(d2esave, 'd2e', panel_data)\n",
    "panel_data = process_and_merge_data(leadup_dummy, 'leadup', panel_data)\n",
    "panel_data = process_and_merge_data(followup_dummy, 'followup', panel_data)\n",
    "panel_data = process_and_merge_data(rv, '5day_std', panel_data)\n",
    "panel_data = process_and_merge_data(vrv, 'std_of_std', panel_data)\n",
    "panel_data = process_and_merge_data(cr, 'cum_returns', panel_data)\n",
    "panel_data = process_and_merge_data(ra, 'rolling_avg', panel_data)\n",
    "\n",
    "panel_data = add_time_varying_factor(sp_path, 'spr', panel_data)\n",
    "\n",
    "panel_data = panel_data.sort_index()\n",
    "panel_data = panel_data.dropna()\n",
    "panel_data.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "#-Naive categorical industry-#\n",
    "industry_path = 'data/data_for_analysis_panel_industry.csv'\n",
    "industry_data = pd.read_csv(get_file_path(industry_path))\n",
    "industry_data['Industry'] = industry_data['Industry'].astype('category')\n",
    "panel_data = panel_data.reset_index()\n",
    "panel_data = pd.merge(panel_data, industry_data[['ISIN', 'Industry']], on='ISIN', how='left')\n",
    "panel_data.set_index(['ISIN', 'Date'], inplace=True)\n",
    "#---------------------------#\n",
    "\n",
    "melt_save_path = 'data/melted_panel.csv'\n",
    "panel_data.to_csv(get_file_path(melt_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "melt_path = 'data/melted_panel.csv'\n",
    "panel_data = pd.read_csv(get_file_path(melt_path), parse_dates=['Date'], index_col=['ISIN', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to include this simplistic model as a demonstration of why it was necessary to pursue a more in depth analysis. The original code for the creation of this data file is not mentioned in this overview because, whilst it is still availible upon request, it evolved into the modules that are now showcased, and is therefore not easily includable in this overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Effects on pre-strike returns:\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0323      0.022      1.485      0.139      -0.011       0.075\n",
      "idle         1.01e-06   3.47e-06      0.291      0.771   -5.82e-06    7.84e-06\n",
      "duration      -0.0003      0.000     -1.413      0.159      -0.001       0.000\n",
      "sp_pre         1.2369      0.359      3.446      0.001       0.529       1.944\n",
      "==============================================================================\n",
      "\n",
      "Effects on returns during the strike:\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0103      0.015      0.699      0.485      -0.019       0.039\n",
      "idle       -2.194e-06   2.33e-06     -0.942      0.347   -6.78e-06     2.4e-06\n",
      "duration       0.0006      0.000      4.706      0.000       0.000       0.001\n",
      "sp_during      0.4267      0.188      2.264      0.025       0.055       0.798\n",
      "==============================================================================\n",
      "\n",
      "Effects on post-strike returns:\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0136      0.018      0.775      0.439      -0.021       0.048\n",
      "idle        2.611e-07   2.73e-06      0.096      0.924   -5.12e-06    5.64e-06\n",
      "duration    3.281e-08      0.000      0.000      1.000      -0.000       0.000\n",
      "sp_post        1.1938      0.307      3.892      0.000       0.589       1.798\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(get_file_path('data/data_for_analysis_naive.csv'))\n",
    "\n",
    "print('\\nEffects on pre-strike returns:')\n",
    "X = sm.add_constant(df[['idle', 'duration', 'sp_pre']])  \n",
    "y = df['pre']\n",
    "res = sm.OLS(y, X).fit()\n",
    "print(res.summary().tables[1])\n",
    "\n",
    "print('\\nEffects on returns during the strike:')\n",
    "X = sm.add_constant(df[['idle', 'duration', 'sp_during']])\n",
    "y = df['during']\n",
    "res = sm.OLS(y, X).fit()\n",
    "print(res.summary().tables[1])\n",
    "\n",
    "print('\\nEffects on post-strike returns:')\n",
    "X = sm.add_constant(df[['idle', 'duration', 'sp_post']])\n",
    "y = df['post']\n",
    "res = sm.OLS(y, X).fit()\n",
    "print(res.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive panel regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn our attention to panel regression. We already know not to include debt-to-equity and current ratio in the same specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                 return   R-squared:                     6.483e-06\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.0218\n",
      "No. Observations:              766100   R-squared (Within):            -8.61e-07\n",
      "Date:                Sun, Dec 10 2023   R-squared (Overall):           6.483e-06\n",
      "Time:                        18:50:03   Log-likelihood                 4.902e+05\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      0.8277\n",
      "Entities:                         100   P-value                           0.5481\n",
      "Avg Obs:                       7661.0   Distribution:                F(6,766093)\n",
      "Min Obs:                       7661.0                                           \n",
      "Max Obs:                       7661.0   F-statistic (robust):             1026.5\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                    7661   Distribution:                F(6,766093)\n",
      "Avg Obs:                      100.000                                           \n",
      "Min Obs:                      100.000                                           \n",
      "Max Obs:                      100.000                                           \n",
      "                                                                                \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "formula = 'return - spr ~ 1 + leadup + strike + followup + ridle + cintensity + d2e'\n",
    "mod = PanelOLS.from_formula(formula, panel_data)\n",
    "res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "print(res.summary.tables[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we note that significantly imporved the imporved F-statistic results, this is a more reasonable estimation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0012     0.0004     3.3696     0.0008      0.0005      0.0019\n",
      "leadup         0.0011     0.0013     0.8427     0.3994     -0.0014      0.0036\n",
      "strike         0.0010     0.0007     1.4302     0.1527     -0.0004      0.0023\n",
      "followup       0.0004     0.0011     0.3864     0.6992     -0.0017      0.0026\n",
      "ridle          0.0002  1.607e-05     13.204     0.0000      0.0002      0.0002\n",
      "cintensity    -0.0008     0.0006    -1.2760     0.2020     -0.0020      0.0004\n",
      "d2e           -0.0001     0.0001    -1.4048     0.1601     -0.0003   5.743e-05\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(res.summary.tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all non-event periods to only consider the strike period +- 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_data['ra'] = panel_data['return'].shift(periods=1).rolling(window=15).mean()\n",
    "strike_mask = panel_data['leadup'] + panel_data['strike'] + panel_data['followup']\n",
    "numeric_cols = panel_data.select_dtypes(include=[np.number])\n",
    "numeric_cols = numeric_cols.multiply(strike_mask, axis=0)\n",
    "non_numeric_cols = panel_data.select_dtypes(exclude=[np.number])\n",
    "filtered_panel_data = pd.concat([non_numeric_cols, numeric_cols], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panel with filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Variable    VIF\n",
      "0       const 1.0197\n",
      "1      leadup 1.9000\n",
      "2      strike 3.0187\n",
      "3    followup 1.9245\n",
      "4       ridle 1.0272\n",
      "5  cintensity 2.3744\n",
      "6         d2e 4.0633\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Assuming 'filtered_panel_data' is your DataFrame and contains all the necessary variables\n",
    "variables = filtered_panel_data[['leadup', 'strike', 'followup', 'ridle', 'cintensity', 'd2e']]\n",
    "\n",
    "# Adding a constant term for bias\n",
    "X = sm.add_constant(variables)\n",
    "\n",
    "# Calculating VIF for each variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                 return   R-squared:                        0.0011\n",
      "Estimator:                   PanelOLS   R-squared (Between):              0.0335\n",
      "No. Observations:              766100   R-squared (Within):               0.0011\n",
      "Date:                Sun, Dec 10 2023   R-squared (Overall):              0.0011\n",
      "Time:                        18:50:06   Log-likelihood                 2.589e+06\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      139.58\n",
      "Entities:                         100   P-value                           0.0000\n",
      "Avg Obs:                       7661.0   Distribution:                F(6,765994)\n",
      "Min Obs:                       7661.0                                           \n",
      "Max Obs:                       7661.0   F-statistic (robust):             813.76\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                    7661   Distribution:                F(6,765994)\n",
      "Avg Obs:                      100.000                                           \n",
      "Min Obs:                      100.000                                           \n",
      "Max Obs:                      100.000                                           \n",
      "                                                                                \n",
      "================================================================================\n",
      "2.072935451610221\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# df = filtered_panel_data\n",
    "# filtered_panel_data = df[(df['strike'] != 0) | (df['ridle'] != 0) | (df['cintensity'] != 0)]\n",
    "\n",
    "\n",
    "# zeries = pd.read_csv(get_file_path('data_for_analysis_panel_5day_average.csv'), index_col='Date').mean(axis=0)\n",
    "# adf_result = adfuller(zeries, autolag='AIC')\n",
    "# print('ADF: ', adf_result[0])\n",
    "\n",
    "formula = 'return - spr ~ leadup + strike + followup + ridle + cintensity + d2e + EntityEffects'\n",
    "mod = PanelOLS.from_formula(formula, filtered_panel_data)\n",
    "res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "print(res.summary.tables[0])\n",
    "print(durbin_watson(res.resids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R squared increases significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "leadup         0.0030     0.0021     1.3901     0.1645     -0.0012      0.0071\n",
      "strike         0.0029     0.0017     1.7016     0.0888     -0.0004      0.0063\n",
      "followup       0.0025     0.0019     1.3207     0.1866     -0.0012      0.0063\n",
      "ridle          0.0002   1.48e-05     16.145     0.0000      0.0002      0.0003\n",
      "cintensity    -0.0006     0.0005    -1.2198     0.2225     -0.0017      0.0004\n",
      "d2e           -0.0009     0.0008    -1.1257     0.2603     -0.0023      0.0006\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(res.summary.tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most p-values suggest statistical insignificance, lets adjust the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.413e-05   2.79e-06     5.0659     0.0000   8.665e-06    1.96e-05\n",
      "ridle          0.0002  4.576e-06     50.732     0.0000      0.0002      0.0002\n",
      "cintensity     0.0004     0.0001     3.3742     0.0007      0.0002      0.0007\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "formula = 'return - spr ~ 1 + ridle + cintensity + EntityEffects'\n",
    "mod = PanelOLS.from_formula(formula, filtered_panel_data)\n",
    "res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "print(res.summary.tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to include categorical industry variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Parameter Estimates                                                     \n",
      "=============================================================================================================================\n",
      "                                                           Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "-----------------------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                                     0.0004     0.0003     1.4834     0.1380     -0.0001      0.0009\n",
      "ridle                                                         0.0002  4.621e-06     50.233     0.0000      0.0002      0.0002\n",
      "cintensity                                                    0.0004     0.0001     3.4470     0.0006      0.0002      0.0007\n",
      "C(Industry)[T.Construction]                                  -0.0004     0.0003    -1.4649     0.1429     -0.0009      0.0001\n",
      "C(Industry)[T.Health Care and Social Assistance]             -0.0004     0.0003    -1.3756     0.1689     -0.0009      0.0002\n",
      "C(Industry)[T.Information]                                   -0.0004     0.0003    -1.6350     0.1020     -0.0010   8.818e-05\n",
      "C(Industry)[T.Manufacturing]                                 -0.0004     0.0003    -1.4613     0.1439     -0.0009      0.0001\n",
      "C(Industry)[T.Mining, Quarrying and Oil & Gas Extraction]    -0.0004     0.0003    -1.4597     0.1444     -0.0009      0.0001\n",
      "C(Industry)[T.Personal Serv & Private Organizations]         -0.0004     0.0003    -1.5419     0.1231     -0.0010      0.0001\n",
      "C(Industry)[T.Retail Trade]                                  -0.0004     0.0003    -1.4468     0.1480     -0.0009      0.0001\n",
      "C(Industry)[T.Support Serv. & Waste Management]              -0.0004     0.0003    -1.5089     0.1313     -0.0009      0.0001\n",
      "C(Industry)[T.Transportation and Warehousing]                -0.0004     0.0003    -1.4174     0.1564     -0.0009      0.0001\n",
      "C(Industry)[T.Utilities]                                     -0.0004     0.0003    -1.5742     0.1154     -0.0010      0.0001\n",
      "=============================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "formula = 'return - spr ~ 1 + ridle + cintensity + C(Industry)'\n",
    "mod = PanelOLS.from_formula(formula, filtered_panel_data)\n",
    "res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "print(res.summary.tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than ideal significance, lets try regressing each industry individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression by industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Regression Results for Industry: Transportation and Warehousing ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   3.116e-06  1.533e-06     2.0323     0.0421   1.108e-07   6.122e-06\n",
      "ridle          0.3579     0.1056     3.3903     0.0007      0.1510      0.5648\n",
      "cintensity     0.0023     0.0021     1.1120     0.2662     -0.0018      0.0064\n",
      "d2e           -0.0016     0.0011    -1.4507     0.1469     -0.0038      0.0006\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Manufacturing ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.921e-06  3.136e-06     0.9316     0.3515  -3.225e-06   9.067e-06\n",
      "ridle          0.0002  9.365e-06     24.233     0.0000      0.0002      0.0002\n",
      "cintensity     0.0004     0.0003     1.2876     0.1979     -0.0002      0.0010\n",
      "d2e         8.979e-05     0.0002     0.4586     0.6466     -0.0003      0.0005\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Construction ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept  -7.167e-07   5.88e-07    -1.2189     0.2229  -1.869e-06   4.358e-07\n",
      "ridle         -0.0237     0.0018    -13.062     0.0000     -0.0272     -0.0201\n",
      "cintensity    -0.0016     0.0002    -10.338     0.0000     -0.0019     -0.0013\n",
      "d2e            0.0014  8.497e-05     16.373     0.0000      0.0012      0.0016\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Accommodation and Food Services ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0004     0.0003     1.4110     0.1583     -0.0002      0.0011\n",
      "ridle       6.686e-05     0.0146     0.0046     0.9963     -0.0285      0.0287\n",
      "cintensity     0.0189     0.0002     117.16     0.0000      0.0186      0.0192\n",
      "d2e           -0.0089     0.0005    -17.938     0.0000     -0.0098     -0.0079\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Retail Trade ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   8.634e-07  2.254e-06     0.3831     0.7016  -3.554e-06   5.281e-06\n",
      "ridle         -0.0431     0.0037    -11.670     0.0000     -0.0503     -0.0359\n",
      "cintensity -1.632e-05     0.0004    -0.0463     0.9631     -0.0007      0.0007\n",
      "d2e            0.0009     0.0003     3.0530     0.0023      0.0003      0.0014\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Health Care and Social Assistance ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   -4.37e-07   1.78e-06    -0.2455     0.8061  -3.926e-06   3.052e-06\n",
      "ridle          0.0015     0.0002     6.8677     0.0000      0.0011      0.0019\n",
      "cintensity     0.0024     0.0003     7.0896     0.0000      0.0018      0.0031\n",
      "d2e            0.0004  1.352e-05     31.651     0.0000      0.0004      0.0005\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Utilities ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept  -5.935e-07  4.218e-07    -1.4071     0.1594   -1.42e-06   2.332e-07\n",
      "ridle          0.2493  1.853e-05  1.345e+04     0.0000      0.2492      0.2493\n",
      "cintensity    -0.0027  1.449e-07 -1.862e+04     0.0000     -0.0027     -0.0027\n",
      "d2e            0.0088  1.812e-07  4.876e+04     0.0000      0.0088      0.0088\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Mining, Quarrying and Oil & Gas Extraction ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.167e-06   2.54e-06     0.8532     0.3935  -2.811e-06   7.145e-06\n",
      "ridle         -0.0167     0.0093    -1.7999     0.0719     -0.0349      0.0015\n",
      "cintensity     0.0050     0.0003     19.668     0.0000      0.0045      0.0055\n",
      "d2e           -0.0021     0.0002    -10.918     0.0000     -0.0025     -0.0017\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Support Serv. & Waste Management ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.641e-20  1.767e-20     0.9287     0.3531  -1.823e-20   5.106e-20\n",
      "ridle      -2.111e-07     0.0242 -8.724e-06     1.0000     -0.0474      0.0474\n",
      "cintensity    -0.0001  4.261e-05    -3.1995     0.0014     -0.0002  -5.281e-05\n",
      "d2e        -3.673e-05     0.0003    -0.1236     0.9017     -0.0006      0.0005\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Personal Serv & Private Organizations ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept  -5.698e-19   5.84e-19    -0.9756     0.3293  -1.715e-18   5.751e-19\n",
      "ridle       1.421e-07     0.0187  7.579e-06     1.0000     -0.0368      0.0368\n",
      "cintensity     0.0003  3.374e-05     8.5304     0.0000      0.0002      0.0004\n",
      "d2e         3.483e-05     0.0002     0.1722     0.8633     -0.0004      0.0004\n",
      "==============================================================================\n",
      "\n",
      "--- Regression Results for Industry: Information ---\n",
      "\n",
      "                             Parameter Estimates                              \n",
      "==============================================================================\n",
      "            Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   3.726e-18  3.737e-18     0.9971     0.3188  -3.599e-18   1.105e-17\n",
      "ridle       3.372e-05     0.0097     0.0035     0.9972     -0.0189      0.0190\n",
      "cintensity     0.0027     0.0001     22.007     0.0000      0.0024      0.0029\n",
      "d2e           -0.0173  1.829e-15 -9.475e+12     0.0000     -0.0173     -0.0173\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "unique_industries = filtered_panel_data['Industry'].unique()\n",
    "industry_estimates = pd.DataFrame(index=unique_industries)\n",
    "\n",
    "for industry in unique_industries:\n",
    "    print(f\"\\n--- Regression Results for Industry: {industry} ---\\n\")\n",
    "    industry_data = filtered_panel_data[filtered_panel_data['Industry'] == industry]\n",
    "\n",
    "    formula = 'return - spr ~ 1 + ridle + cintensity + d2e'\n",
    "    mod = PanelOLS.from_formula(formula, industry_data, check_rank=False, drop_absorbed=True)\n",
    "    res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "\n",
    "    industry_estimates.loc[industry, 'strike'] = res.params.get('ridle', float('nan'))\n",
    "    industry_estimates.loc[industry, 's_ridle'] = res.std_errors.get('ridle', float('nan'))\n",
    "\n",
    "    print(res.summary.tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking pooled regression further with interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivankhalin/anaconda3/lib/python3.11/site-packages/linearmodels/panel/model.py:1214: MissingValueWarning: \n",
      "Inputs contain missing values. Dropping rows with missing observations.\n",
      "  super().__init__(dependent, exog, weights=weights, check_rank=check_rank)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PanelOLS Estimation Summary                           \n",
      "================================================================================\n",
      "Dep. Variable:                 return   R-squared:                        0.0216\n",
      "Estimator:                   PanelOLS   R-squared (Between):             -0.6042\n",
      "No. Observations:              766085   R-squared (Within):               0.0216\n",
      "Date:                Sun, Dec 10 2023   R-squared (Overall):              0.0216\n",
      "Time:                        18:50:13   Log-likelihood                 2.597e+06\n",
      "Cov. Estimator:             Clustered                                           \n",
      "                                        F-statistic:                      2118.3\n",
      "Entities:                         100   P-value                           0.0000\n",
      "Avg Obs:                       7660.9   Distribution:                F(8,765977)\n",
      "Min Obs:                       7646.0                                           \n",
      "Max Obs:                       7661.0   F-statistic (robust):             4268.8\n",
      "                                        P-value                           0.0000\n",
      "Time periods:                    7661   Distribution:                F(8,765977)\n",
      "Avg Obs:                       99.998                                           \n",
      "Min Obs:                       99.000                                           \n",
      "Max Obs:                      100.000                                           \n",
      "                                                                                \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "formula = 'return - spr ~ 1 + leadup + strike + ridle + ra + ra:ridle + leadup:ridle + ridle:spr + strike:spr + EntityEffects'\n",
    "\n",
    "mod = PanelOLS.from_formula(formula, filtered_panel_data)\n",
    "res = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "print(res.summary.tables[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Parameter Estimates                               \n",
      "================================================================================\n",
      "              Parameter  Std. Err.     T-stat    P-value    Lower CI    Upper CI\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept     8.537e-06  7.169e-06     1.1909     0.2337  -5.514e-06   2.259e-05\n",
      "leadup           0.0014     0.0008     1.7514     0.0799     -0.0002      0.0030\n",
      "strike           0.0009     0.0005     1.8576     0.0632  -4.845e-05      0.0018\n",
      "ridle            0.0014  1.855e-05     77.323     0.0000      0.0014      0.0015\n",
      "ra              -0.3164     0.0457    -6.9246     0.0000     -0.4059     -0.2268\n",
      "ra:ridle        -0.0204     0.0006    -34.193     0.0000     -0.0215     -0.0192\n",
      "leadup:ridle    -0.0014  2.909e-05    -47.720     0.0000     -0.0014     -0.0013\n",
      "ridle:spr       -0.0395     0.0032    -12.164     0.0000     -0.0458     -0.0331\n",
      "strike:spr       0.7042     0.0803     8.7692     0.0000      0.5468      0.8616\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(res.summary.tables[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
